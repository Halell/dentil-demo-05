תודה על הלוג – עכשיו ברור איפה עוד “נתפס” בלוק 2.
אני מציע **Fix Pack v3** קצר וממוקד שיסגור את שלושת הפערים המרכזיים שעדיין רואים אצלך:

1. **אל תאגד n‑gram שחוצה מספרים/זוגות** → אין יותר mentions כמו `“שלת 14 18/0”`.
2. **אליאס→OHD אמיתי** (לא `DEVICE:`/`IMPLANT:` פלייסהולדרים).
3. **ניקוד היברידי עם re‑weight** + שמירת `surface` המקורי (לא מחליפים ל“שתל”).

להלן המשימות המדויקות (copy‑paste לאייג’נט), ואח״כ “מה צפוי להשתנות בפלט שלך”.

---

## 🔧 Fix Pack v3 — משימות לביצוע

### 1) גנרטור n‑gram שלא חוצה מספרים/זוגות

**קובץ:** `src/pipeline/t2_gazetteer.py`

* בנה חלונות n‑gram **רק** מעל טווחים של טוקנים `kind=="word"`. שבור רצפים בכל פעם שיש `kind in {"number","pair","unit"}`.
* אל תייצר mention עם `grouped_span` אם החלון כולל לא‑word.

```python
ALLOWED_FOR_NGRAM = {"word"}

def word_segments(tokens):
    seg, curr = [], []
    for i,t in enumerate(tokens):
        if t.kind in ALLOWED_FOR_NGRAM:
            curr.append(i)
        else:
            if curr: seg.append(curr); curr=[]
    if curr: seg.append(curr)
    return seg

# ואז בתוך כל segment תריץ n-grams (1..NGRAM_MAX), עם longest-match-wins ובלי חפיפות.
```

* **Guard** נוסף לפני הוספת mention:

```python
if any(tokens[k].kind in {"number","pair","unit"} for k in covered_token_idxs):
    continue  # אל תיצור mention
```

> זה ימנע בדיוק את ה־`mention_id: m0_10_13_g` עם `surface: "שלת 14 18/0"`.

---

### 2) שמור `surface` מקורי; הוסף `normalized_surface` לשדות העזר

**קבצים:** `schemas.py`, `t2_gazetteer.py`

* בסכימת מועמד הוסף:

```python
"normalized_surface": "שתל"  # אם נעשתה נרמול פאזי מ"שלת"
"match_type": "label|synonym|fuzzy"
```

* **אסור להחליף** `surface` המקורי. אם פענחת “שלת”→“שתל”, דווח זאת רק ב‑`normalized_surface`.

---

### 3) Resolver לאליאסים → IRI אמיתי של OHD

**קובץ חדש:** `src/pipeline/alias_resolver.py`

```python
def resolve_alias_to_ohd(alias_text, lexicon_entries, faiss_index, embedder):
    # 1) exact/synonym match (case-insensitive for EN)
    # 2) containment match על label/synonyms מנורמלים
    # 3) vector Top-3 -> בחר הגבוה שה-label/סינונימיו הכי קרובים לטקסט
    # return {"iri": ohd_iri, "label": ohd_label, "source": "resolved_alias"} or None
```

**שילוב בגזטיר (`t2_gazetteer.py`):**

* אם `en_alias_to_iri.json` מחזיר **טקסט אנגלי** (לא URL), קרא `resolve_alias_to_ohd(...)`.
* אם הצליח → החלף את ה־candidate ל‑IRI האמיתי של OHD והצב `iri_source="resolved_alias"`.
* אם לא הצליח → `iri_source="alias_only"` ותן עונש ציון קטן (ראה סעיף 5).

---

### 4) חיזוק Hints נכונים

**קובץ:** `t2_gazetteer.py`

* מפה פשוטה:

```python
if surface in {"מולטיוניט","MU","MUA"} or normalized_surface in {"multi unit abutment","multi-unit abutment"}:
    hints.add("device_hint")
if surface in {"שתל"} or normalized_surface in {"dental implant"}:
    hints.add("implant_hint")
```

* אל תשאיר `hints: []` ל“מולטיוניט” — זה אמור להיות `device_hint`.

---

### 5) היבריד: ניקוד + re‑weight + ענישה ל‑alias\_only

**קובץ:** `src/pipeline/hybrid_ranker.py`

* **אל תנרמל** `score_lex` (הוא כבר 0..1).
* `norm_vec = normalize_scores(vec_scores)`; אם אין וקטורים → `vec=None`.
* **Re‑weight** רק על הרכיבים הקיימים:

```python
parts = {"lex": score_lex, "vec": norm_vec if vec_present else None, "prior": prior, "ctx": ctx}
present = {k:v for k,v in parts.items() if v is not None}
Z = sum(W[k] for k in present)
score = sum(W[k]*present[k] for k in present) / max(Z, 1e-9)
```

* אם `candidate.iri_source == "alias_only"` → `score -= 0.08` (לא פחות מ‑0).
* `confident_singleton=True` רק אם:

  * `len(kept)==1`
  * `score_lex≥0.9`
  * `iri_source in {"ohd_label","ohd_synonym","resolved_alias"}`
  * `covered_token_idxs` קיימים ורק `word`‑tokens.

---

### 6) בדיקת FAISS בפועל (כדי לייצר גם vec)

**קובץ:** `cand_faiss.py`

* לוג דיבאג קצר:

```python
logger.debug(f"[vec] query='{query_text}' top3={[(iri,score) for iri,score in top3]}")
```

* `build_query_text`: קודם `canonical_terms` (אם יהיו מ‑LLM‑Rescue בהמשך), אחרת `he2en_static[surface]`, אחרת `surface`; הוסף 1–2 שכנים (מילים בלבד).

---

### 7) (אופציונלי) LLM‑Rescue בטוח – HE→EN בלבד

רק אם לטוקן `word` **אין בכלל** candidates (לא lex, לא vec):

**קובץ:** `src/pipeline/llm_rescue.py`

```python
def rescue_canonical_terms(surface_he: str) -> list[str]|None:
    # החזר JSON {"canonical_terms_en": ["multi-unit abutment"]} בלבד
    # ולידציה: regex ^[A-Za-z][A-Za-z -]{0,40}$, עד 3 מילים, אין ספרות/יחידות
```

* לאחר מכן נסה שוב FAISS עם ה‑terms.
* אין הקצאת IRI ע״י ה‑LLM — רק מחרוזת שאילתה.

---

## ✅ Acceptance מהיר (על הקלט שלך)

**קלט:** `מולטיוניט שלת14 18/0`

צפוי אחרי ה‑Fix Pack:

* **Gazetteer**

  * mention A: `surface="מולטיוניט"`, `span=[0,9]`, `hints=["device_hint"]`,
    candidates: `iri=<OHD ... dental implant abutment>`, `iri_source="resolved_alias"`, `score_lex=1.0`.
  * mention B: `surface="שלת"`, `span=[10,13]`, `hints=["implant_hint"]`,
    candidates: `iri=<OHD ... dental implant>`, `normalized_surface="שתל"`, `match_type="fuzzy"`, `score_lex≈0.75–0.85`.
  * **אין** mention ל־`"שלת 14 18/0"`; **אין** mentions למספרים/Pair.

* **Vector**

  * יופיעו `norm_vec>0` לשני ה‑mentions (הודות ל‑he2en/aliases).

* **Hybrid**

  * “מולטיוניט”: `score_final≈0.9–0.96`, `confident_singleton=True`.
  * “שלת”: `score_final≈0.7–0.85`, `confident_singleton` כנראה False (תלוי תוצאות vec).

* **MV (v2)**

  * שתי רשומות בלבד, `IRI`‑ים אמיתיים של OHD, `surface` מקורי, `spans` מדויקים.

---

## למה זה יפתור בדיוק את מה שראית

* חסימת n‑gram מעל מספרים/זוגות → לא עוד `grouped_span`.
* Alias‑resolver → לא נשארים עם `DEVICE:`/`IMPLANT:`; מקבלים OHD אמיתי.
* Re‑weight → גם בלי vec, ציון סופי לא “נתקע” על 0.6/0.55.
* Hints נכונים + `surface` מקורי → עקיבות מלאה ומוכנות ל‑2.5/3.

כשתריץ שוב ותראה שני אזכורים עם IRIs אמיתיים וציון סופי חיובי — נעבור ל‑**Block 2.5 (Pruner & Bundles)** ואח”כ ל‑**Block 3** בבטחה.
